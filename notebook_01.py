# -*- coding: utf-8 -*-
"""NoteBook_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmrQH1w9AIMA-6Qma-KMj84-1XMzM0ii
"""

pip install sklearn

#importing libraries

import pandas as pd                     
import matplotlib.pyplot as plt        
from sklearn import tree
import numpy as np 
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge 
from sklearn.linear_model import Lasso 
from sklearn.ensemble import RandomForestRegressor  
from sklearn import svm   
from sklearn.tree import DecisionTreeRegressor   
from sklearn.neighbors import KNeighborsClassifier    
from sklearn.preprocessing import StandardScaler
from sklearn import neighbors
from sklearn.metrics import mean_squared_error 
from math import sqrt
from sklearn.metrics import r2_score
from sklearn import metrics
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense

#importing the dataset

data = pd.read_excel("path_to_dataset")

#getting the information of data

print(data.info())

#describing the dataset

df.describe().transpose()

"""**Preprocessing of data**"""

#replacing special characters with NULL value

df = data.replace('[@_!#$%^&*()<>?/\|}{~:]',np.NaN, regex=True)
print("\n\n", df)

#datasplitting into numeric & object columns

object_columns_df = df.select_dtypes(include=['object'])
numerical_columns_df =df.select_dtypes(exclude=['object'])

#Number of null values in object feature

null_counts = object_columns_df.isnull().sum()
print("Number of null values in each column:\n{}".format(null_counts))

#list of numericl features

numerical_columns_df.dtypes

#checking the number of null values in numerical features

null_counts = numerical_columns_df.isnull().sum()
print("Number of null values in each column:\n{}".format(null_counts))

#Filling NULL value by fillna_method

df['total_area']=df['total_area'].fillna(value=df['total_area'].mean(), inplace=False)
df['furnished']=df['furnished'].fillna(value=df['furnished'].mode()[0], inplace=False)
df['lot_measure15']=df['lot_measure15'].fillna(value=df['lot_measure15'].mean(), inplace=False)
df['living_measure15']=df['living_measure15'].fillna(value=df['living_measure15'].mean(), inplace=False)
df['ceil_measure']=df['ceil_measure'].fillna(value=df['ceil_measure'].mean(), inplace=False)
df['living_measure']=df['living_measure'].fillna(value=df['living_measure'].median(), inplace=False)
df['lot_measure']=df['lot_measure'].fillna(value=df['lot_measure'].mean(), inplace=False)
df['quality']=df['quality'].fillna(value=df['quality'].mean(), inplace=False)
df['basement']=df['basement'].fillna(value=df['basement'].mean(), inplace=False)
df['condition']=df['condition'].fillna(value=df['condition'].mode()[0], inplace=False)
df['ceil']=df['ceil'].fillna(value=df['ceil'].mode()[0], inplace=False)
df['coast']=df['coast'].fillna(value=df['coast'].mode()[0], inplace=False)
df['sight']=df['sight'].fillna(value=df['sight'].mode()[0], inplace=False)
df['long']=df['long'].fillna(value=None, method='ffill',inplace=False)
df['yr_built']=df['yr_built'].fillna(value=None, method='ffill',inplace=False)
df['room_bed']=df['room_bed'].fillna(value=2, method=None,inplace=False)
df['room_bath']=df['room_bath'].fillna(value=1, method=None,inplace=False)

"""Treatment of Outliars"""

#the boxplot of feature to get insight about outliar

sns.boxplot(df['room_bath'],data=df)

#getting quartile of feature

print(df['room_bath'].quantile(0.50)) 
print(df['room_bath'].quantile(0.95))

#replacement of outliars with median

df['room_bath'] = np.where(df['room_bath'] > 3.5, 2.25, df['room_bath'])

#boxplot after removal of outliars

sns.boxplot(df['room_bath'],data=df)

#boxplot for outliar detection

sns.boxplot(df['ceil'],data=df)                   #no outliar is identified as shown by boxplot

"""Exploratory Data Analysis(EDA)"""

#Using Pearson Correlation

plt.figure(figsize=(15,12))
cor = df.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

#Transformation of Date

df['dayhours']= [x.strip().replace('T000000','') for x in df.dayhours]
df['dayhours'] = pd.to_datetime(df.dayhours)
df['year_sold'] = df.dayhours.dt.year
df_1 = df.drop(columns = 'dayhours')

df_1[df_1['room_bed'] == 33].index
df_1['room_bed']=df_1['room_bed'].astype('category')
df_1.drop(columns = 'cid',inplace=True)
df_1.drop(index=750,inplace=True)
df_1 = df_1.reset_index()
df_1.drop(columns='index',inplace=True)

df_1['room_bath']=df_1['room_bath'].astype('category')
df_1['ceil']=df_1['ceil'].astype('category')
df_1['coast']=df_1['coast'].astype('category')
df_1['sight']=df_1['sight'].astype('category')
df_1['condition']=df_1['condition'].astype('category')
df_1['quality']=df_1['quality'].astype('category')
df_1['have_basement'] = df_1['basement'].apply(lambda x: 0 if x==0 else 1)
df_1['Is_renovated'] = df_1['yr_renovated'].apply(lambda x: 0 if x==0 else 1)
df_1['Age_of_house'] = df_1['year_sold'] - df_1['yr_built']

lst = []
for i in df_1.lat:
    if i<47.255900:
        lst.append('ES')
    elif i>47.255900 and i<47.405900:
        lst.append('MS')
    elif i>47.405900 and i<47.555900:
        lst.append('MN')
    else:
        lst.append('EN')
df_1['SN_region'] = lst
df_1['SN_region'] = df_1['SN_region'].astype('category')

lst = []
for i in abs(df_1.long):
    if i<122.105000:
        lst.append('EE')
    elif i>122.105000 and i<122.205000:
        lst.append('ME')
    elif i>122.205000 and i<122.328000:
        lst.append('MW')
    else:
        lst.append('EW')
df_1['EW_region'] = lst
df_1['EW_region'] = df_1['EW_region'].astype('category')

delete_index = df_1[df_1['Age_of_house']== -1]['year_sold'].index
df_1.drop(index=delete_index,inplace = True)

#the distribution of each feature

df.hist(figsize=(25,20))
plt.show()

#building of trends for features

list1 = ['dayhours','cid','price','long', 'lat',
        'year_sold', 'yr_built', 'sight', 'basement','yr_renovated',
        'furnished','zipcode','living_measure15','lot_measure15']
df_def = df.drop(columns=list1)
def trend():
    for i in df_def.columns:
        df[[i,'price']].groupby([i]).sum().plot(figsize=(15,5))
        plt.show()

trend()

#ploting of scatter plot

list1 = ['dayhours','cid','price','room_bed', 'room_bath',
        'ceil', 'coast', 'sight', 'condition', 'quality',
        'furnished','zipcode','year_sold','yr_renovated','lat','long','living_measure15','lot_measure15']
df_def = df.drop(columns=list1)
def scatterr():
    for i in df_def.columns:
        plt.figure(figsize=(15,5))
        sns.regplot(x='price',y=i, data=df, color='#79d13e')
        plt.show()

scatterr()

#visualizing square footage of (home,lot,above and basement)

fig = plt.figure(figsize=(16,5))
fig.add_subplot(2,2,1)
sns.scatterplot(df['living_measure'], data['price'])
fig.add_subplot(2,2,2)
sns.scatterplot(df['lot_measure'],data['price'])
fig.add_subplot(2,2,3)
sns.scatterplot(df['ceil_measure'],data['price'])
fig.add_subplot(2,2,4)
sns.scatterplot(df['basement'],data['price'])

#visualizing house prices

fig = plt.figure(figsize=(10,7))
fig.add_subplot(2,1,1)
sns.distplot(df['price'])

"""Feature Selection for Modeling"""

#Correlation with output variable

cor_target = abs(cor["price"])

#Selecting highly correlated features

selected_features = []
relevant_features = cor_target[cor_target>=0.2]
for i in relevant_features.keys():
  selected_features.append(i)
selected_features.remove('price')

#checking preprocessed data for null values

final_df = df[relevant_features.keys()]

final_df.isnull().sum()

#datatype of each feature in data

display(final_df.dtypes)

#conversion of datatype of features

final_df['room_bed'] = pd.to_numeric(final_df['room_bed'], errors = 'coerce')
final_df['room_bed'] = final_df['room_bed'].astype('int64')

final_df['room_bath'] = pd.to_numeric(final_df['room_bath'], errors = 'coerce')
final_df['room_bath'] = final_df['room_bath'].astype('int64')

final_df['living_measure'] = pd.to_numeric(final_df['living_measure'], errors = 'coerce')
final_df['living_measure'] = final_df['living_measure'].astype('int64')

final_df['ceil'] = pd.to_numeric(final_df['ceil'], errors = 'coerce')
final_df['ceil'] = final_df['ceil'].astype('int64')

final_df['sight'] = pd.to_numeric(final_df['sight'], errors = 'coerce')
final_df['sight'] = final_df['sight'].astype('int64')

final_df['coast'] = pd.to_numeric(final_df['coast'], errors = 'coerce')
final_df['coast'] = final_df['coast'].astype('int64')

final_df['quality'] = pd.to_numeric(final_df['quality'], errors = 'coerce')
final_df['quality'] = final_df['quality'].astype('int64')

final_df['ceil_measure'] = pd.to_numeric(final_df['ceil_measure'], errors = 'coerce')
final_df['ceil_measure'] = final_df['ceil_measure'].astype('int64')

final_df['basement'] = pd.to_numeric(final_df['basement'], errors = 'coerce')
final_df['basement'] = final_df['basement'].astype('int64')

final_df['lat'] = pd.to_numeric(final_df['lat'], errors = 'coerce')
final_df['lat'] = final_df['lat'].astype('int64')

final_df['living_measure15'] = pd.to_numeric(final_df['living_measure15'], errors = 'coerce')
final_df['living_measure15'] = final_df['living_measure15'].astype('int64')


final_df['furnished'] = pd.to_numeric(final_df['furnished'], errors = 'coerce')
final_df['furnished'] = final_df['furnished'].astype('int64')

#display of datatype of each feature

display(final_df.dtypes)

#split dataset in features and target variable

Features = selected_features
X = final_df[Features] # Features
y = final_df.price # Target variable

# split X and y into training and testing sets

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20, random_state=25)

#scaling of each feature

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

#standardization scaler - fit&transform on train, fit only on test

s_scaler = StandardScaler()
X_train = s_scaler.fit_transform(X_train.astype(np.float))
X_test = s_scaler.transform(X_test.astype(np.float))

#Optimal number of clusters for KNN

rmse_val = []                   #to store rmse values for different k
for K in range(20):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K)

    model.fit(X_train, y_train)                    #fit the model
    pred=model.predict(X_test)                     #make prediction on test set
    error = sqrt(mean_squared_error(y_test,pred))  #calculate root mean square error(rmse)
    rmse_val.append(error)                         #store root mean square error(rmse) values
    print('RMSE value for k= ' , K , 'is:', error)

#plotting the rmse values against k values(elbow curve)

curve = pd.DataFrame(rmse_val)                    
curve.plot()

""" **Predictive Models**"""

#1. KNN Model

knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

acc_knn = metrics.explained_variance_score(y_test, knn_pred)
print('MAE:', metrics.mean_absolute_error(y_test, knn_pred))  
print('MSE:', metrics.mean_squared_error(y_test, knn_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, knn_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,knn_pred))

# 2. OLS

ols = LinearRegression()
ols.fit(X_train, y_train)
ols_pred = ols.predict(X_test)

acc_ols = metrics.explained_variance_score(y_test, ols_pred)
print('MAE:', metrics.mean_absolute_error(y_test, ols_pred))  
print('MSE:', metrics.mean_squared_error(y_test, ols_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, ols_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,ols_pred))

# 3. Ridge

ridge = Ridge(alpha=0.5, fit_intercept=True,normalize=False, copy_X=True, max_iter=1000, tol=0.001, 
 solver='auto', random_state=None)
ridge.fit(X_train, y_train)
ridge_pred = ridge.predict(X_test)

acc_ridge = metrics.explained_variance_score(y_test, ridge_pred)
print('MAE:', metrics.mean_absolute_error(y_test, ridge_pred))  
print('MSE:', metrics.mean_squared_error(y_test, ridge_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, ridge_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,ridge_pred))

# 4. Lasso

lasso = Lasso(alpha=0.01, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, 
            tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
lasso.fit(X_train, y_train)
lasso_pred = lasso.predict(X_test)

acc_lasso = metrics.explained_variance_score(y_test, lasso_pred)
print('MAE:', metrics.mean_absolute_error(y_test, lasso_pred))  
print('MSE:', metrics.mean_squared_error(y_test, lasso_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, lasso_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,lasso_pred))

# 5. Random Forest

rf = RandomForestRegressor(max_depth=10,random_state=0,n_estimators=1000, n_jobs=-1)
rf.fit(X_train,y_train)
rf_pred = rf.predict(X_test)

acc_rf = metrics.explained_variance_score(y_test, rf_pred)
print('MAE:', metrics.mean_absolute_error(y_test, rf_pred))  
print('MSE:', metrics.mean_squared_error(y_test, rf_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, rf_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,rf_pred))

# 6. SVM Regression

reg = svm.SVR()
reg.fit(X_train, y_train)
reg_pred = reg.predict(X_test)

acc_reg = metrics.explained_variance_score(y_test, reg_pred)
print('MAE:', metrics.mean_absolute_error(y_test, reg_pred))  
print('MSE:', metrics.mean_squared_error(y_test, reg_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, reg_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,reg_pred))

# 7. Decision Tree

DT = DecisionTreeRegressor(max_depth = 5)
DT.fit(X_train, y_train)
DT_pred = reg.predict(X_test)

acc_DT = metrics.explained_variance_score(y_test, DT_pred)
print('MAE:', metrics.mean_absolute_error(y_test, DT_pred))  
print('MSE:', metrics.mean_squared_error(y_test, DT_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, DT_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,DT_pred))

#building the neural network model

nn_model = Sequential()

nn_model.add(Dense(50, activation='relu'))
nn_model.add(Dense(100, activation='relu'))
nn_model.add(Dense(50, activation='relu'))
nn_model.add(Dense(1))

nn_model.compile(loss='mean_squared_error', optimizer='adam')

#training the neural network model

hist=nn_model.fit(x=X_train,y=y_train,
          batch_size=40,epochs=400,
    shuffle=True,
    verbose=2
)
nn_model.summary()

#prediction of neural network model on test dataset

nn_pred = nn_model.predict(X_test)

acc_nn = metrics.explained_variance_score(y_test, nn_pred)
print('MAE:', metrics.mean_absolute_error(y_test, nn_pred))  
print('MSE:', metrics.mean_squared_error(y_test, nn_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, nn_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,nn_pred))

#comparison of models for variance score

models = pd.DataFrame({
    'Model': ['knn','Linear Regression', 'Ridge','Lasso','Random Forest', 'Support Vector Machines','Decision Tree', 'Neural Network'],
    'VarScore': [acc_knn*100,acc_ols*100, acc_ridge*100, acc_lasso*100,acc_rf*100,acc_reg*100, acc_DT*100, acc_nn*100]})
models.sort_values(by='VarScore', ascending=False)

# XGBoost Regressor

from xgboost import XGBRegressor
xgboost = XGBRegressor(learning_rate=0.01,
                                            n_estimators=6000,
                                            max_depth=4,
                                            min_child_weight=0,
                                            gamma=0.6,
                                            subsample=0.7,
                                            colsample_bytree=0.7,
                                            objective='reg:linear',
                                            nthread=-1,
                                            scale_pos_weight=1,
                                            seed=27,
                                            reg_alpha=0.00006,
                                            random_state=42,
                      n_jobs=-1)

xgboost.fit(X_train,y_train)
xgboost_pred = xgboost.predict(X_test)

acc_xgboost = metrics.explained_variance_score(y_test, xgboost_pred)
print('MAE:', metrics.mean_absolute_error(y_test, xgboost_pred))  
print('MSE:', metrics.mean_squared_error(y_test, xgboost_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, xgboost_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,xgboost_pred))

#Bagging Regressor

from sklearn.ensemble import BaggingRegressor
br_model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))
br_model.fit(X_train, y_train)
br_pred = br_model.predict(X_test)


acc_br = metrics.explained_variance_score(y_test, xgboost_pred)
print('MAE:', metrics.mean_absolute_error(y_test, xgboost_pred))  
print('MSE:', metrics.mean_squared_error(y_test, xgboost_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, xgboost_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,xgboost_pred))

#ensembling method for prediction

ensemble_pred = (lasso_pred + rf_pred +  ridge_pred +
           ols_pred )/4

acc_ensemble = metrics.explained_variance_score(y_test, ensemble_pred)
print('MAE:', metrics.mean_absolute_error(y_test, ensemble_pred))  
print('MSE:', metrics.mean_squared_error(y_test, ensemble_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, ensemble_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,ensemble_pred))

#comparison of models for variance score

models = pd.DataFrame({
    'Model': ['knn','Linear Regression', 'Ridge','Lasso','Random Forest', 'Support Vector Machines','Decision Tree', 'Neaural Network', 'XgBoost','BaggingRegressor','Ensemble Model'],
    'VarScore': [acc_knn*100,acc_ols*100, acc_ridge*100, acc_lasso*100,acc_rf*100,acc_reg*100, acc_DT*100, acc_nn*100, acc_xgboost*100, acc_br*100, acc_ensemble*100]})
models.sort_values(by='VarScore', ascending=False)

# plot between predictions and Y_test

x_axis = np.array(range(0, nn_pred.shape[0]))
plt.figure(figsize=(20,10))
plt.plot(x_axis, rf_pred, linestyle="--", marker="o", alpha=0.7, color='r', label="predictions")
plt.plot(x_axis, y_test, linestyle="--", marker="o", alpha=0.7, color='g', label="Y_test")
plt.xlabel('Row number')
plt.ylabel('PRICE')
plt.title('Predictions vs Y_test')
plt.legend(loc='lower right')

#plottinf for the importance of features

feature_importance = rf.feature_importances_
feature_importance = 100.0 * (feature_importance / feature_importance.max())

sorted_idx = np.argsort(feature_importance)
pos        = np.arange(sorted_idx.shape[0]) + .5

plt.figure(figsize = (12,15))

#Make a horizontal bar plot.

plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, df.columns[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')

#regularization 

from keras.layers import Dropout
from keras import regularizers

#model building 

model_3 = Sequential([
                       

    Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    
    Dropout(0.3),
    Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1, kernel_regularizer=regularizers.l2(0.01)),
])

#compiling the parameters  of model

model_3.compile(optimizer='adam',
              loss='mean_squared_error')

#fitting the model on training dataset

hist_3 = model_3.fit(x=X_train,y=y_train,
          batch_size=32,epochs=100,
    shuffle=True,
    verbose=2)

model_3.summary()

#prediction on test dataset

model_3_pred = model_3.predict(X_test)

acc_nn = metrics.explained_variance_score(y_test, model_3_pred)
print('MAE:', metrics.mean_absolute_error(y_test, model_3_pred))  
print('MSE:', metrics.mean_squared_error(y_test, model_3_pred))  
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, model_3_pred)))
print('VarScore:',metrics.explained_variance_score(y_test,model_3_pred))